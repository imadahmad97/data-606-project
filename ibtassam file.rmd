```{r}
library(dplyr)
library(sampling)
library(tree)
library(MASS)
library(ISLR)
raw = read.csv('chess_df_overall.csv')
```


## Ibtassam code Q1 CLASSIFICATION

```{r}
train2 = read.csv("q1_final_training_data.csv")
train2
```


```{r}
# Convert categorical variables to factors
train2$op_name = as.factor(train2$op_name)
train2$victory_status = as.factor(train2$victory_status)
train2$pace_type = as.factor(train2$pace_type)
train2$rated = as.factor(train2$rated)
train2$rating_bins = as.factor(train2$rating_bins)
train2$W1 = as.factor(train2$W1)
train2$B1 = as.factor(train2$B1)
train2$opening_strategy = as.factor(train2$opening_strategy)
```



factor(op_name) can't have more than 32 levels as per error so I excluded it from below tree
```{r}
tree.class2<-tree(factor(winner)~rated+turns+pace_type+W1+B1+Rating_WB+WOE_Opening_Name, train2)
summary(tree.class2)
```

plot the tree.


```{r}
library(rpart.plot)
tree_model2 = rpart(factor(winner)~rated+turns+pace_type+W1+B1+WOE_Opening_Name+opening_strategy+rating_bins, data = train2, method="class")
rpart.plot(tree_model2, type = 2, extra =1, under = TRUE, branch.lty=3, digits = 4)
```


```{r}
plot(tree.class2)
text(tree.class2, pretty=0)
```






# Ibtassam Code Q2 CLASSIFICATION


Classification tree & k-fold

```{r}
train = read.csv("q1_final_training_data.csv")
train
```


```{r}
train$victory_status = as.factor(train$victory_status)
train$pace_type = as.factor(train$pace_type)
train$rated = as.factor(train$rated)
train$rating_bins = as.factor(train$rating_bins)
train$W1 = as.factor(train$W1)
train$B1 = as.factor(train$B1)
```


```{r}
contrasts(train$pace_type)
```


```{r}
library(rpart)
library(rpart.plot)
tree_model = rpart(factor(victory_status)~factor(rated)+turns+factor(pace_type)+Rating_WB+factor(rating_bins)+factor(W1)+factor(B1)+factor(opening_strategy), data = train, method="class")
#prp(tree_model)
rpart.plot(tree_model, type = 2, extra =1, under = TRUE, branch.lty=3, digits = 4) #extra = 1 gives n, 100 gives %age, extra = 100 gives %age of n


#tree.class<-tree(factor(victory_status)~factor(rated)+turns+factor(pace_type)+Rating_WB+factor(rating_bins)+factor(W1)+factor(B1), train)
#summary(tree.class)
```

Step 3: plot the tree.
```{r}
plot(tree.class)
text(tree.class, pretty=0)
```

<!-- Apply the fitted tree to the test set -->
<!-- ```{r} -->
<!-- tree.pred<-predict(tree.class,test,type = "class") -->
<!-- table(tree.pred,test$HD) -->
<!-- ``` -->

<!-- prune the tree using cross-validation (We use -->
<!-- the argument ${\color{red}{FUN=prune.misclass}}$ in order to indicate that we want the -->
<!-- classification error rate to guide the cross-validation and pruning process) -->
<!-- ```{r} -->
<!-- set.seed(3) -->
<!-- cv.class<-cv.tree(tree.class, FUN = prune.misclass, K=10)  -->
<!-- plot(cv.class$size, cv.class$dev,type="b") -->
<!-- ``` -->

<!-- find the "best" tree using the cross-validation result -->
<!-- ```{r} -->
<!-- prune.class=prune.tree(tree.class,best=6) -->
<!-- plot(prune.class) -->
<!-- text(prune.class,pretty=0) -->
<!-- ``` -->

<!-- apply the pruned tree to do prediction -->
<!-- ```{r} -->
<!-- prune.pred=predict(prune.class,test,type="class") -->
<!-- table(prune.pred,test$HD) -->
<!-- ``` -->


























